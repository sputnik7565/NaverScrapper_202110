{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40588d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=========================================================================\n",
      "==================== 게임 카페 게시판 크롤러 ver 1.0 ====================\n",
      "\n",
      "           본 데이터는 최신게임과 커뮤니티의 규모를 고려하여\n",
      "            네이버 카페 게임 섹션의 급상승 Top100 리스트와\n",
      "       실시간 Top100 리스트를 비교, 두 리스트에 공통으로 포함된 \n",
      "                   카페의 게시판 데이터를 추출합니다.\n",
      "             \n",
      "=========================================================================\n",
      "데이터를 추출할 기간을 입력해 주세요: 1\n",
      "1일간의 수집된 데이터를 추출시작합니다.\n",
      "급상승 Top100과 랭킹 Top100에 모두 포함된 카페의 수는 30 건 입니다.\n",
      "=========================================================================\n",
      "<---------1번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------2번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------3번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------4번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------5번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------6번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------7번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------8번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------9번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------10번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------11번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------12번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------13번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------14번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------15번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------16번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------17번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------18번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------19번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------20번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------21번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------22번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------23번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------24번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------25번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------26번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------27번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------28번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------29번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "<---------30번째 카페 데이터 추출을 마쳤습니다.--------->\n",
      "엑셀 파일을 csv로 변환작업 중입니다.\n",
      "=========================================================================\n",
      "총 소요시간은 1134.9 초입니다.\n",
      "파일 저장 경로는 C:\\Users\\sputnik\\Python\\WebScrapper\\NaverScrapper_202110\\Log\\ 으로 추출을 마치고 프로그램을 종료합니다.\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "#모듈 로딩\n",
    "#### 모듈 및 라이브러리 로딩\n",
    "from bs4 import BeautifulSoup #뷰티풀숲 로딩\n",
    "from selenium import webdriver #셀레니움 드라이버 로딩\n",
    "from selenium.webdriver.common.keys import Keys #키에 관련된 모듈 가져오기\n",
    "import time #시간 체크 및 관리를 위한 모듈 로딩\n",
    "\n",
    "import sys #파일로 저장하기 위한 모듈\n",
    "import os #폴더를 다루기 위한 모듈\n",
    "import pandas as pd #데이터 프레임으로 만들기 위한 모듈\n",
    "import xlwt #엑셀로 저장을 위해 \n",
    "import openpyxl#엑셀 파일을 다루기 위한 모듈\n",
    "import csv #csv 모듈\n",
    "\n",
    "import re #정규식 모듈 임포트, 파일명 잘못 생성시 예외처리를 위한 모듈\n",
    "from datetime import datetime#날짜 계산을 위한 타임모듈\n",
    "\n",
    "\n",
    "#사용자 입력 받는 부분과, 최초 실행시간 코드 지정\n",
    "print('''\n",
    "=========================================================================\n",
    "==================== 게임 카페 게시판 크롤러 ver 1.0 ====================\n",
    "\n",
    "           본 데이터는 최신게임과 커뮤니티의 규모를 고려하여\n",
    "            네이버 카페 게임 섹션의 급상승 Top100 리스트와\n",
    "       실시간 Top100 리스트를 비교, 두 리스트에 공통으로 포함된 \n",
    "                   카페의 게시판 데이터를 추출합니다.\n",
    "             \n",
    "=========================================================================''')\n",
    "\n",
    "\n",
    "#입력값이 비었이면 False, 아니면 True-------------------------------------------------------\n",
    "def isNotBlank (myString):\n",
    "    return bool(myString and myString.strip())\n",
    "\n",
    "#수집할 기간 입력 --------------------------------------------------------------------------\n",
    "while True:\n",
    "    try:\n",
    "        limit_day =int(input('데이터를 추출할 기간을 입력해 주세요: '))  \n",
    "    except ValueError:\n",
    "        print('양의 정수 중에서 입력해 주세요.')\n",
    "    else:\n",
    "        if limit_day==0:\n",
    "            limit_day = 1\n",
    "            print(\"입력값이 없다면 최소단위인 1일간의 수집된 데이터를 추출시작합니다.\")\n",
    "        else:\n",
    "            limit_day = abs(limit_day)\n",
    "            print('%s일간의 수집된 데이터를 추출시작합니다.'%limit_day)\n",
    "        break\n",
    "        \n",
    "\n",
    "#크롤링 시작 시간을 위한 타임스탬프를 찍습니다.\n",
    "s_time = time.time()\n",
    "\n",
    "\n",
    "#브라우저 오픈 및 정렬 기준 변경 -------------------------------------------------------------\n",
    "def open_br():\n",
    "    global driver\n",
    "    global path\n",
    "\n",
    "    path = 'chromedriver.exe'\n",
    "    driver = webdriver.Chrome(path)\n",
    "    driver.get(\"https://section.cafe.naver.com/ca-fe/home/themes\")#네이버 카페 주제별 링크\n",
    "    time.sleep(1)\n",
    "    \n",
    "    #정렬기준 멤버순 변경\n",
    "    driver.find_element_by_xpath ( '// button [normalize-space () = \"급상승 순\"]').click()\n",
    "    time.sleep(0.5)\n",
    "    driver.find_element_by_xpath ( '// button [normalize-space () = \"멤버 순\"]').click()    \n",
    "    time.sleep(0.5)\n",
    "        \n",
    "open_br()\n",
    "\n",
    "#주제별 카페에서 급상승 Top100의 카페 이름과 URL을 수집하는 클래스-----------------------------\n",
    "class CrawlingTop100:\n",
    "    def __init__(self, dic):\n",
    "        self.dic = dic\n",
    "        #dic = dict()\n",
    "        self.active_code_end = True\n",
    "        \n",
    "    def crawling_page(self): \n",
    "        self.no = 1\n",
    "        for x in range(1, 7):\n",
    "            time.sleep(1)\n",
    "            self.html = driver.page_source#페이지 전체를 가져옴\n",
    "            self.soup = BeautifulSoup(self.html, 'html.parser')\n",
    "            self.content_list = self.soup.find('ul', 'common_list').find_all('li')\n",
    "\n",
    "            for i in self.content_list:\n",
    "                #print('번호:', self.no)\n",
    "                self.title = i.find('div', 'list_info').find('div', 'name_area').get_text()\n",
    "                #print(self.title.strip())\n",
    "                self.url = i.find('a', 'list_link')['href']\n",
    "                #print(self.url)      \n",
    "\n",
    "                self.dic[self.no] = [self.title, self.url]\n",
    "                self.no+=1\n",
    "            time.sleep(1)\n",
    "\n",
    "            self.page_num= str(x+1)\n",
    "            if (x > 0) and (x < 6) :\n",
    "                driver.find_element_by_link_text(self.page_num).click()\n",
    "            x += 1\n",
    "            \n",
    "            if self.no == 100:\n",
    "                break     \n",
    "        return self.dic\n",
    "        \n",
    "        return self.active_code_end\n",
    "    \n",
    "    \n",
    "    \n",
    "#Top100들에서 타이틀과 URL 수집------------------------------------------------------\n",
    "dic_a = {}\n",
    "dic_b = {}\n",
    "crwling_top100_a = CrawlingTop100(dic_a)\n",
    "crwling_top100_b = CrawlingTop100(dic_b)\n",
    "\n",
    "crwling_top100_a.crawling_page()\n",
    "\n",
    "#급상승 끝나면 랭킹으로 이동\n",
    "if crwling_top100_a.active_code_end == True:\n",
    "    driver.find_element_by_link_text('랭킹 Top100').click()\n",
    "    time.sleep(1)\n",
    "    crwling_top100_b.crawling_page()\n",
    "    \n",
    "    \n",
    "    \n",
    "#같은 값의 주소리스트와 갯수 수집--------------------------------------------------\n",
    "list_a = []\n",
    "list_b = []\n",
    "for i in dic_a:\n",
    "    title = dic_a[i][1]\n",
    "    list_a.append(title)\n",
    "for i in dic_b:\n",
    "    title = dic_b[i][1]\n",
    "    list_b.append(title)\n",
    "\n",
    "list_ab = [val for val in list_a if val in list_b]\n",
    "\n",
    "\n",
    "print('급상승 Top100과 랭킹 Top100에 모두 포함된 카페의 수는 %s 건 입니다.'%str(len(list_ab)))\n",
    "print('='*73)\n",
    "\n",
    "\n",
    "#각 카페에서 데이터 수집하는 클래스(no)---------------------------------------------\n",
    "class CrawlingCafe:\n",
    "    def __init__(self, url, limit_day):\n",
    "        self.url = url\n",
    "        self.limit_day = limit_day\n",
    "        \n",
    "    def crawling_cafe(self):\n",
    "        driver.get(self.url)\n",
    "        \n",
    "        self.html = driver.page_source\n",
    "        self.soup = BeautifulSoup(self.html, 'html.parser')\n",
    "         \n",
    "        self.g_name = []\n",
    "        self.gc_name = []\n",
    "        self.gc_url = []\n",
    "\n",
    "        self.gc_total_member = []\n",
    "\n",
    "        self.gc_total_board_count = []\n",
    "\n",
    "        self.gc_board_num = []\n",
    "        self.gc_board_category = []\n",
    "        self.gc_board_date = []\n",
    "        self.gc_board_title = []\n",
    "        \n",
    "        self.rg_name = self.soup.find('title').get_text()\n",
    "        self.re_rg_name = re.sub('[\\\\\\/:*?\"<>|\\]\\[-]','',self.rg_name)\n",
    "        self.rg_name = re.sub('공식.*\\Z|네이버.*\\Z|커뮤니티.*\\Z|카페.*\\Z','',self.re_rg_name)#공식 뒤의 문자를 모두 날림\n",
    "\n",
    "        self.rgc_name = self.soup.find('title').get_text()\n",
    "        \n",
    "        self.rgc_url = driver.current_url#현재 브라우저 URL\n",
    "\n",
    "        self.rgc_total_member = self.soup.find('li', 'mem-cnt-info').find('a').get_text()\n",
    "        self.rgc_total_member = re.sub('비공개','', self.rgc_total_member)\n",
    "        self.rgc_total_member = self.rgc_total_member.replace('\\n', '')\n",
    "\n",
    "        self.rgc_total_board_count = self.soup.find('ul', 'cafe-menu-list').find('li').find('span').get_text()                  \n",
    "        \n",
    "       \n",
    "        driver.find_element_by_link_text('전체글보기').click()\n",
    "\n",
    "                \n",
    "                \n",
    "        time.sleep(0.5)\n",
    "        \n",
    "            \n",
    "        self.today = datetime.today().strftime('%Y-%m-%d')\n",
    "        self.re_today = re.sub('-', '', self.today)             \n",
    "\n",
    "        #----------자유게시판 iframe이동----------------\n",
    "        driver.switch_to.frame(\"cafe_main\")\n",
    "\n",
    "        self.no = 1\n",
    "        self.x = 1\n",
    "        while True:\n",
    "            time.sleep(0.5)\n",
    "            self.iframe = driver.page_source\n",
    "            self.soup = BeautifulSoup(self.iframe, 'html.parser')\n",
    "            self.board = self.soup.find_all('div', 'article-board m-tcol-c')\n",
    "            self.board2 = self.board[1].find('tbody').find_all('tr')\n",
    "\n",
    "            for i in self.board2:              \n",
    "                try:\n",
    "                    self.title = i.find('div','board-list').find('div','inner_list').find('a', 'article').get_text()\n",
    "                except:\n",
    "                    continue\n",
    "                else:\n",
    "                    self.title = self.title.replace(\"\\n\",\"\")\n",
    "                    self.title = self.title.replace(\"\\t\",\"\")\n",
    "\n",
    "                try:\n",
    "                    self.w_time = i.find('td','td_date').get_text()\n",
    "                except:\n",
    "                    continue\n",
    "                else:\n",
    "                    self.w_time = self.w_time.replace(\"\\n\",\"\")\n",
    "                    self.w_time = self.w_time.replace(\"\\t\",\"\")\n",
    "                    #기간설정을 위한 w_time 변환\n",
    "                    self.re_time = re.sub('[:.]','',self.w_time)\n",
    "                    \n",
    "                try:\n",
    "                    self.category = i.find('div','board-name').find('div','inner_name').find('a').get_text()\n",
    "                except:\n",
    "                    continue\n",
    "                else:\n",
    "                    self.category = self.category.replace(\"\\n\",\"\")\n",
    "                    self.category = self.category.replace(\"\\t\",\"\")                                      \n",
    "                \n",
    "                #기간 설정 조건 비교 후 크롤링 끝내기 ----------                   \n",
    "                if self.limit_day == 1:\n",
    "                    if len(self.re_time) > 4:\n",
    "                        break\n",
    "                else:\n",
    "                    self.crawling_days = int(self.re_today) - self.limit_day\n",
    "                    if int(self.re_time) == self.crawling_days:\n",
    "                        break\n",
    "                #-----------------------------------------------\n",
    "\n",
    "                self.g_name.append(self.rg_name)\n",
    "                self.gc_name.append(self.rgc_name)\n",
    "                self.gc_url.append(self.rgc_url)\n",
    "                self.gc_total_member.append(self.rgc_total_member)\n",
    "                self.gc_total_board_count.append(self.rgc_total_board_count)\n",
    "                \n",
    "                  \n",
    "                #print(self.title)\n",
    "                self.gc_board_title.append(self.title.strip())\n",
    "                #print(self.w_time)\n",
    "                self.gc_board_date.append(self.w_time.strip())      \n",
    "                self.gc_board_category.append(self.category.strip())\n",
    "                self.no += 1\n",
    "\n",
    "            self.page_num= str(self.x+1)\n",
    "            self.list_num = self.page_num\n",
    "\n",
    "            if (self.x % 10) == 0:\n",
    "                driver.find_element_by_link_text(\"다음\").click()\n",
    "            else:\n",
    "                driver.find_element_by_link_text(self.page_num).click()\n",
    "            self.x += 1\n",
    "\n",
    "\n",
    "            #기간 설정 조건 비교 후 크롤링 끝내기 ----------\n",
    "            #기간설정을 위한 w_time 변환\n",
    "            self.re_time = re.sub('[:.]','',self.w_time)                \n",
    "            if self.limit_day == 1:\n",
    "                if len(self.re_time) > 4:\n",
    "                    break\n",
    "            else:\n",
    "                self.crawling_days = int(self.re_today) - self.limit_day\n",
    "                if int(self.re_time) == self.crawling_days:\n",
    "                    break       \n",
    "        return self.g_name, self.gc_name, self.gc_url, self.gc_total_member, self.gc_total_board_count, self.gc_board_date, self.gc_board_title, self.gc_board_category\n",
    "\n",
    "    \n",
    "    \n",
    "#저장될 파일명 생성, 폴더 변경 ------------------------------------------------------------------------------------\n",
    "now = time.localtime()\n",
    "f_name = '%04d-%02d-%02d-%02d-%02d-%02d' %(now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min, now.tm_sec)\n",
    "\n",
    "fc_name = f_name+'-'+\"gamecafe_data\"+'.csv'\n",
    "fx_name = f_name+'-'+\"gamecafe_data\"+'.xlsx'\n",
    "final_f_dir = os.getcwd()+\"\\\\Log\\\\\"   \n",
    "\n",
    "#폴더 생성 시작\n",
    "try:\n",
    "    os.makedirs(final_f_dir)\n",
    "except FileExistsError:#동일 폴더 생성시 예외처리\n",
    "    pass\n",
    "\n",
    "os.chdir(final_f_dir)\n",
    "\n",
    "\n",
    "#반복 카페 글 크롤링 후 저장 ---------------------------------------------------------------------------------------\n",
    "writer = pd.ExcelWriter(fx_name, engine='xlsxwriter')\n",
    "column_width = 20\n",
    "\n",
    "no = 1\n",
    "\n",
    "for i in range(len(list_ab)):\n",
    "    i = CrawlingCafe(list_ab[i], limit_day)\n",
    "    i.crawling_cafe()\n",
    "    \n",
    "    cafe_data = \"cafe_data\"+str(i)\n",
    "    cafe_data = pd.DataFrame()\n",
    "    cafe_data['게임 명'] = i.g_name\n",
    "    cafe_data['카페 명'] = i.gc_name\n",
    "    cafe_data['카페 주소'] = i.gc_url\n",
    "    cafe_data['카페 멤버수'] = i.gc_total_member\n",
    "    cafe_data['카페 총게시물 수'] = i.gc_total_board_count\n",
    "    cafe_data['글 번호'] = [i+1 for i in range(len(i.gc_board_title))]\n",
    "    cafe_data['게시판 이름'] = i.gc_board_category\n",
    "    cafe_data['글 작성 시간'] = i.gc_board_date\n",
    "    cafe_data['글 내용'] = i.gc_board_title\n",
    "    \n",
    "    cafe_data.to_excel(writer, sheet_name = i.g_name[0], index=False)    \n",
    "    worksheet = writer.sheets[i.g_name[0]]\n",
    "    worksheet.set_column(4, 0, column_width)\n",
    "\n",
    "    print(\"<---------\"+str(no)+\"번째 카페 데이터 추출을 마쳤습니다.--------->\")\n",
    "    #print(cafe_data)\n",
    "    time.sleep(2)\n",
    "    no+=1\n",
    "writer.save()\n",
    "\n",
    "e_time = time.time()\n",
    "t_time = e_time - s_time #크롤링에 쓰인 시간\n",
    "\n",
    "#엑셀 파일 csv 시트별로 저장----------------------------------------------------\n",
    "def convert_csv():\n",
    "    print('엑셀 파일을 csv로 변환작업 중입니다.')\n",
    "    time.sleep(4)\n",
    "\n",
    "    sheetList = []\n",
    "    #엑셀의 열어 workbook을 얻어옵니다. \n",
    "    wb = openpyxl.load_workbook(final_f_dir + fx_name)\n",
    "    xlsxFile =final_f_dir + fx_name\n",
    "\n",
    "    for i in wb.sheetnames:\n",
    "        sheetList.append(i)\n",
    "\n",
    "    xlsx = pd.ExcelFile(xlsxFile)\n",
    "\n",
    "    no = 1\n",
    "    for j in sheetList:\n",
    "        df = pd.read_excel(xlsx, j)\n",
    "        df.to_csv(fx_name[:-5]+'_sheet_{}'.format(no)+'.csv', encoding='utf-8-sig', index=False)\n",
    "        no+=1\n",
    "\n",
    "convert_csv()\n",
    "\n",
    "print('='*73)\n",
    "print('총 소요시간은 %s 초입니다.'%round(t_time, 1))\n",
    "print('파일 저장 경로는 %s 으로 추출을 마치고 프로그램을 종료합니다.'%final_f_dir)\n",
    "print('='*73)\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daccbd6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
